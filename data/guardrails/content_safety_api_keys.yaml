# version: 1.0.3
---
prompts:
  - task: content_safety_check_input $model=content_safety
    content: |
      Task: Check if there is unsafe content in the user or agent messages in the below conversation in accordance with our safety policy with the below categories. Provide a 'safe' or 'unsafe' rating for both the user message and the agent response. If there is unsafe content, please also provide a list of violated categories according to our safety policy below.

      <BEGIN UNSAFE CONTENT CATEGORIES>
      S1: Violence.
      S2: Sexual.
      S3: Criminal Planning/Confessions.
      S4: Guns and Illegal Weapons.
      S5: Controlled/Regulated Substances.
      S6: Suicide and Self Harm.
      S7: Sexual (minor).
      S8: Hate/Identity Hate.
      S9: PII/Privacy.
      S10: Harassment.
      S11: Threat.
      S12: Profanity.
      S13: Needs Caution.
      S14: Other.
      S15: Manipulation.
      S16: Fraud/Deception.
      S17: Malware.
      S18: High Risk Gov Decision Making.
      S19: Political/Misinformation/Conspiracy.
      S20: Copyright/Trademark/Plagiarism.
      S21: Unauthorized Advice.
      S22: Illegal Activity.
      S23: Immoral/Unethical.
      <END UNSAFE CONTENT CATEGORIES>

      <BEGIN CONVERSATION>

      user: {{ user_input }}

      <END CONVERSATION>

      Provide your safety assessment for the above conversation in the below output JSON format:
      {"User Safety": safe or unsafe rating for the user message, "Response Safety": safe or unsafe rating for the agent response. Omit if no agent response present. "Safety Categories": a comma-separated list of applicable safety categories from the provided taxonomy. Omit if all safe.}

      Do not include anything other than the output JSON in your response.
      Output JSON:
    output_parser: nemoguard_parse_prompt_safety
    max_tokens: 50

  - task: content_safety_check_output $model=content_safety
    content: |
      Task: Check if there is unsafe content in the user or agent messages in the below conversation in accordance with our safety policy with the below categories. Provide a 'safe' or 'unsafe' rating for both the user message and the agent response. If there is unsafe content, please also provide a list of violated categories according to our safety policy below.

      <BEGIN UNSAFE CONTENT CATEGORIES>
      S1: Violence.
      S2: Sexual.
      S3: Criminal Planning/Confessions.
      S4: Guns and Illegal Weapons.
      S5: Controlled/Regulated Substances.
      S6: Suicide and Self Harm.
      S7: Sexual (minor).
      S8: Hate/Identity Hate.
      S9: PII/Privacy.
      S10: Harassment.
      S11: Threat.
      S12: Profanity.
      S13: Needs Caution.
      S14: Other.
      S15: Manipulation.
      S16: Fraud/Deception.
      S17: Malware.
      S18: High Risk Gov Decision Making.
      S19: Political/Misinformation/Conspiracy.
      S20: Copyright/Trademark/Plagiarism.
      S21: Unauthorized Advice.
      S22: Illegal Activity.
      S23: Immoral/Unethical.
      <END UNSAFE CONTENT CATEGORIES>

      <BEGIN CONVERSATION>

      user: {{ user_input }}

      response: agent: {{ bot_response }}

      <END CONVERSATION>

      Provide your safety assessment for the above conversation in the below output JSON format:
      {"User Safety": safe or unsafe rating for the user message, "Response Safety": safe or unsafe rating for the agent response. Omit if no agent response present. "Safety Categories": a comma-separated list of applicable safety categories from the provided taxonomy. Omit if all safe.}

      Do not include anything other than the output JSON in your response.
      Output JSON:
    output_parser: nemoguard_parse_response_safety
    max_tokens: 50

  - task: content_safety_check_input $model=llama_guard
    content: |
      <s>[INST] Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.
      <BEGIN UNSAFE CONTENT CATEGORIES>
      O1: Violence and Hate.
      ...
      O7: Offensive Language and Insults.
      <END UNSAFE CONTENT CATEGORIES>

      <BEGIN CONVERSATION>
      User: {{ user_input }}
      <END CONVERSATION>

      Provide your safety assessment for User in the above conversation:
      - First line must read 'safe' or 'unsafe'.
      - If unsafe, a second line must include a comma-separated list of violated categories. [/INST]
    output_parser: is_content_safe

  - task: content_safety_check_output $model=llama_guard_2
    content: |
      <|begin_of_text|>[INST] Task: Check if there is unsafe content in Agent messages in conversations according our safety policy with the below categories.

      <BEGIN UNSAFE CONTENT CATEGORIES>
      S1: Violent Crimes.
      S2: Non-Violent Crimes.
      S3: Sex Crimes.
      ...
      S11: Sexual Content.
      <END UNSAFE CONTENT CATEGORIES>

      <BEGIN CONVERSATION>

      User: {{ user_input }}

      Agent: {{ bot_response }}

      <END CONVERSATION>

      Provide your safety assessment for ONLY THE LAST Agent in the above conversation:
      - First line must read 'safe' or 'unsafe'.
      - If unsafe, a second line must include a comma-separated list of violated categories. [/INST]
    output_parser: is_content_safe
    max_tokens: 3

  - task: content_safety_check_input $model=shieldgemma
    content: |
      You are a policy expert trying to help determine whether a user prompt is in
      violation of the defined safety policies.

      <start_of_turn>
      Human Question: {{ user_input }}
      <end_of_turn>

      Our safety principle is defined in the below:

      * "No Hate Speech": ...
      * "No Dangerous Content": ...
      * "No Sexually Explicit Information": ...
      * "No Harassment": ...
      ===

      Does the human prompt/question violate the above principle? Your answer must start with
      'Yes' or 'No'. And then walk through step by step to be sure we answer
      correctly.
    output_parser: is_content_safe
    max_tokens: 3

  - task: content_safety_check_output $model=shieldgemma
    content: |
      You are a policy expert trying to help determine whether a chatbot response to the user is in
      violation of the defined safety policies:

      <start_of_turn>
      Human Question: {{ user_input }}
      Chatbot Response: {{ bot_response }}
      <end_of_turn>

      Our safety principle is defined in the below:

      * "No Dangerous Content": ...
      * "No Harassment": ...
      * "No Hate Speech": ...
      * "No Sexually Explicit Information": ...

      ===

      Does the chatbot response violate the above principle? Your answer must start with
      'Yes' or 'No'. And then walk through step by step to be sure we answer
      correctly.
    output_parser: is_content_safe
    max_tokens: 3
